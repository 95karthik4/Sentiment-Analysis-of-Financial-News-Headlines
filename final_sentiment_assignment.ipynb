{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086dad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce49aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER DATASET LOCATION (Mac)\n",
    "DATA_PATH = \"/Users/karthik/Downloads/financial_news_headlines_sentiment.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT DIRECTORY (Mac Downloads)\n",
    "OUT_DIR = \"/Users/karthik/Downloads/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d2c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=DATA_PATH):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(path, header=None, encoding='latin1')\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(\"Expected two columns: label, text\")\n",
    "    df = df.iloc[:, :2]\n",
    "    df.columns = ['label', 'text']\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b332a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "    df['text'] = df['text'].astype(str).str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba37ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_features(X_train, X_test, method=\"count\"):\n",
    "    if method == \"count\":\n",
    "        vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "    Xtr = vectorizer.fit_transform(X_train)\n",
    "    Xte = vectorizer.transform(X_test)\n",
    "    return Xtr, Xte, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed3c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, preds, average='weighted')\n",
    "    return acc, precision, recall, f1, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(y_true, y_pred, title, filename):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, filename))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb592987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df = load_data()\n",
    "    df = preprocess_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84099275",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X = df['text']\n",
    "    y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24693fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d132dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    models = {\n",
    "        \"MultinomialNB\": MultinomialNB(),\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=1000)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    feature_methods = [\"count\", \"tfidf\"]\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa06f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for fm in feature_methods:\n",
    "        Xtr, Xte, vec = vectorize_features(X_train, X_test, fm)\n",
    "        for model_name, model in models.items():\n",
    "            acc, prec, rec, f1, preds = evaluate_model(model, Xtr, y_train, Xte, y_test)\n",
    "            results.append([fm, model_name, acc, prec, rec, f1])\n",
    "            plot_confusion(\n",
    "                y_test, preds,\n",
    "                f\"Confusion Matrix: {model_name} ({fm})\",\n",
    "                f\"confusion_{fm}_{model_name}.png\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e871aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    results_df = pd.DataFrame(results, columns=[\"Features\", \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "    results_df.to_csv(os.path.join(OUT_DIR, \"results_summary.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94379884",
   "metadata": {},
   "outputs": [],
   "source": [
    "    best_row = results_df.sort_values(\"F1\", ascending=False).iloc[0]\n",
    "    summary_text = f\"\"\"Assignment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results Summary:\n",
    "{results_df.to_string(index=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best Method:\n",
    "Feature Type: {best_row['Features']}\n",
    "Model: {best_row['Model']}\n",
    "Weighted F1 Score: {best_row['F1']:.4f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanation:\n",
    "This model achieved the highest weighted F1 score, balancing precision and recall across classes and producing overall superior sentiment classification performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(os.path.join(OUT_DIR, \"assignment_submission.txt\"), \"w\") as f:\n",
    "        f.write(summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e09f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
